{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "\n",
    "**Welcome back, St. Mark!** Building on linear regression, we now tackle classification. Think of this as upgrading from measuring disease severity (regression) to diagnosing disease presence (classification).\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "1. **Binary Logistic Regression** - Sigmoid activation and log-loss\n",
    "2. **Gradient Descent Training** - Adapting our optimization techniques\n",
    "3. **Multiclass Extension** - Softmax for multiple disease categories\n",
    "4. **Decision Boundaries** - Visualizing classification regions\n",
    "\n",
    "By the end, you'll understand how neural networks classify data.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Logistic regression is classification through regression:\n",
    "- **Features (X)**: Input measurements (clinical symptoms)\n",
    "- **Sigmoid Function**: Converts linear outputs to probabilities (0-1)\n",
    "- **Decision Boundary**: Threshold for classification (usually 0.5)\n",
    "- **Log Loss**: Measures classification error instead of squared error\n",
    "\n",
    "**Key Question:** How do we predict disease categories instead of continuous measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Binary Classification Setup\n",
    "\n",
    "We'll use scikit-learn's `make_classification` to create synthetic data mimicking disease diagnosis scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create synthetic binary classification data\n",
    "# n_classes=2 for disease present/absent\n",
    "# n_clusters_per_class=1 for clear separation\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                          n_features=4,\n",
    "                          n_classes=2,\n",
    "                          n_informative=3,\n",
    "                          n_redundant=1,\n",
    "                          n_clusters_per_class=1,\n",
    "                          random_state=42)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add intercept term\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Reshape targets for matrix operations\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training set: X={X_train_b.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set: X={X_test_b.shape}, y={y_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train.flatten())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've prepared our classification dataset.\n",
    "\n",
    "- **Binary labels:** 0 = disease absent, 1 = disease present\n",
    "- **Train/test split:** Prevents overfitting evaluation\n",
    "- **Class balance:** Important for healthcare applications\n",
    "\n",
    "**Healthcare Analogy:** Like preparing diagnostic test data - symptoms as features, disease presence as target.\n",
    "\n",
    "**Reflection Question:** Why is class balance important in Nigerian healthcare AI systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Sigmoid Function - The Probability Converter\n",
    "\n",
    "The sigmoid function transforms linear outputs into probabilities:\n",
    "\n",
    "**œÉ(z) = 1 / (1 + e^(-z))**\n",
    "\n",
    "This creates the S-curve that maps any real number to (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    Converts linear outputs to probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    z: Linear combination of features and weights\n",
    "    \n",
    "    Returns:\n",
    "    Probability between 0 and 1\n",
    "    \"\"\"\n",
    "    # Clip to prevent overflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Visualize sigmoid function\n",
    "z_values = np.linspace(-10, 10, 100)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_values, sigmoid_values, 'b-', linewidth=2, label='Sigmoid')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.7, label='Zero Input')\n",
    "plt.xlabel('Linear Input (z)')\n",
    "plt.ylabel('Probability Output')\n",
    "plt.title('Sigmoid Function: Linear ‚Üí Probability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test sigmoid with sample values\n",
    "test_inputs = [-5, -1, 0, 1, 5]\n",
    "print(\"Sigmoid Test Values:\")\n",
    "for z in test_inputs:\n",
    "    prob = sigmoid(z)\n",
    "    prediction = \"Disease Present\" if prob > 0.5 else \"Disease Absent\"\n",
    "    print(f\"z={z:4d} ‚Üí p={prob:.3f} ‚Üí {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** \n",
    "\n",
    "**Sigmoid Properties:**\n",
    "- **Range:** (0,1) - perfect for probabilities\n",
    "- **S-shape:** Smooth transition around decision boundary\n",
    "- **Asymptotes:** Approaches 0 and 1 but never reaches them\n",
    "\n",
    "**Healthcare Translation:** Converts symptom severity scores into disease probability estimates.\n",
    "\n",
    "**Reflection Question:** How does the sigmoid function help with uncertain diagnoses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Log Loss - Classification Error Measurement\n",
    "\n",
    "Log loss measures classification error using negative log likelihood:\n",
    "\n",
    "**L(y,p) = -[y¬∑log(p) + (1-y)¬∑log(1-p)]**\n",
    "\n",
    "This heavily penalizes confident wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss (log loss).\n",
    "    \n",
    "    Measures classification error in probability space.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: True binary labels (0 or 1)\n",
    "    y_pred_prob: Predicted probabilities [0,1]\n",
    "    \n",
    "    Returns:\n",
    "    Average log loss across all samples\n",
    "    \"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred_prob = np.clip(y_pred_prob, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Binary cross-entropy formula\n",
    "    loss = -(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
    "    \n",
    "    return np.mean(loss)\n",
    "\n",
    "def log_loss_gradients(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradients of log loss w.r.t. parameters.\n",
    "    \n",
    "    Mathematical derivation:\n",
    "    ‚àÇL/‚àÇŒ∏ = (1/m) * X^T * (œÉ(XŒ∏) - y)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Forward pass: compute probabilities\n",
    "    z = X @ theta\n",
    "    predictions = sigmoid(z)\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    errors = predictions - y\n",
    "    gradients = (1/m) * X.T @ errors\n",
    "    \n",
    "    return gradients, predictions\n",
    "\n",
    "# Visualize log loss penalty\n",
    "prob_range = np.linspace(0.01, 0.99, 100)\n",
    "loss_true_1 = -np.log(prob_range)  # True label = 1\n",
    "loss_true_0 = -np.log(1 - prob_range)  # True label = 0\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(prob_range, loss_true_1, 'r-', label='True label = 1', linewidth=2)\n",
    "plt.plot(prob_range, loss_true_0, 'b-', label='True label = 0', linewidth=2)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Log Loss Penalty Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show extreme penalty for wrong confident predictions\n",
    "wrong_probs = [0.01, 0.1, 0.4]  # Confident wrong predictions\n",
    "for prob in wrong_probs:\n",
    "    loss_1 = -np.log(prob)\n",
    "    loss_0 = -np.log(1-prob)\n",
    "    plt.bar([f'p={prob}\\n(True=1)', f'p={prob}\\n(True=0)'], [loss_1, loss_0], \n",
    "            alpha=0.7, label=f'p={prob}')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Penalty for Confident Wrong Predictions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test log loss with sample predictions\n",
    "test_cases = [\n",
    "    (1, 0.9),  # Correct high confidence\n",
    "    (1, 0.1),  # Wrong high confidence  \n",
    "    (0, 0.5),  # Correct uncertainty\n",
    "    (0, 0.6)   # Wrong slight confidence\n",
    "]\n",
    "\n",
    "print(\"Log Loss Examples:\")\n",
    "for true, pred in test_cases:\n",
    "    loss = -(true * np.log(pred) + (1-true) * np.log(1-pred))\n",
    "    print(f\"True={true}, Pred={pred} ‚Üí Loss={loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Log Loss Properties:**\n",
    "- **Asymmetric penalty:** Wrong confident predictions punished heavily\n",
    "- **Proper scoring rule:** Calibrates probability estimates\n",
    "- **Differentiable:** Enables gradient-based optimization\n",
    "\n",
    "**Healthcare Analogy:** Like medical malpractice - wrong confident diagnoses are severely penalized.\n",
    "\n",
    "**Reflection Question:** Why is log loss better than accuracy for training medical diagnosis AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Binary Logistic Regression Training\n",
    "\n",
    "Now we combine sigmoid activation with gradient descent to train our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X, y, learning_rate=0.1, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Train binary logistic regression using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    X: Feature matrix with intercept\n",
    "    y: Binary target vector\n",
    "    learning_rate: Step size for gradient descent\n",
    "    n_iterations: Maximum training iterations\n",
    "    \n",
    "    Returns:\n",
    "    theta: Learned parameters\n",
    "    losses: Training loss history\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1) * 0.01  # Small random initialization\n",
    "    losses = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Compute gradients and predictions\n",
    "        gradients, predictions = log_loss_gradients(X, y, theta)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "        # Track loss\n",
    "        loss = log_loss(y, predictions)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if iteration > 10 and abs(losses[-1] - losses[-2]) < 1e-6:\n",
    "            print(f\"Converged after {iteration} iterations\")\n",
    "            break\n",
    "            \n",
    "    return theta, losses\n",
    "\n",
    "# Train our logistic regression\n",
    "theta_lr, losses_lr = train_logistic_regression(X_train_b, y_train, \n",
    "                                               learning_rate=0.1,\n",
    "                                               n_iterations=1000)\n",
    "\n",
    "print(f\"Learned parameters: {theta_lr.flatten()}\")\n",
    "print(f\"Final training loss: {losses_lr[-1]:.4f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_lr, 'b-', linewidth=2)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Logistic Regression Training Progress')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Training Process:**\n",
    "- **Initialization:** Small random weights to avoid saturation\n",
    "- **Gradient computation:** Uses log loss derivatives\n",
    "- **Convergence:** Loss decreases smoothly to minimum\n",
    "\n",
    "**Healthcare Translation:** Like training a doctor - starts with random guesses, improves with experience.\n",
    "\n",
    "**Reflection Question:** Why do we initialize weights small in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Multiclass Extension - Softmax Regression\n",
    "\n",
    "For multiple disease categories, we extend to softmax:\n",
    "\n",
    "**Softmax(z_i) = e^(z_i) / Œ£ e^(z_j)**\n",
    "\n",
    "This normalizes outputs into probability distribution over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax activation for multiclass classification.\n",
    "    \n",
    "    Converts raw scores to normalized probabilities.\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_stable)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred_prob):\n",
    "    \"\"\"\n",
    "    Categorical cross-entropy loss for multiclass.\n",
    "    \"\"\"\n",
    "    # Clip for numerical stability\n",
    "    y_pred_prob = np.clip(y_pred_prob, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(y_true * np.log(y_pred_prob)) / y_true.shape[0]\n",
    "\n",
    "# Create multiclass data for demonstration\n",
    "X_multi, y_multi = make_classification(n_samples=600, n_features=4, n_classes=3, \n",
    "                                      n_informative=3, n_redundant=1, \n",
    "                                      n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "def to_one_hot(y, n_classes):\n",
    "    one_hot = np.zeros((y.shape[0], n_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_multi_onehot = to_one_hot(y_multi, 3)\n",
    "\n",
    "# Add intercept\n",
    "X_multi_b = np.c_[np.ones((X_multi.shape[0], 1)), X_multi]\n",
    "\n",
    "# Initialize weights for 3 classes\n",
    "theta_multi = np.random.randn(X_multi_b.shape[1], 3) * 0.01\n",
    "\n",
    "# Forward pass demonstration\n",
    "z_multi = X_multi_b @ theta_multi\n",
    "probabilities = softmax(z_multi)\n",
    "\n",
    "print(\"Sample predictions (first 5 examples):\")\n",
    "print(\"Probabilities shape:\", probabilities.shape)\n",
    "print(\"Sample predictions:\")\n",
    "for i in range(5):\n",
    "    probs = probabilities[i]\n",
    "    predicted_class = np.argmax(probs)\n",
    "    confidence = probs[predicted_class]\n",
    "    print(f\"Example {i+1}: Class {predicted_class} (confidence: {confidence:.3f})\")\n",
    "\n",
    "# Visualize softmax behavior\n",
    "raw_scores = np.array([[2, 1, 0], [0, 1, 2], [1, 1, 1]])\n",
    "softmax_probs = softmax(raw_scores)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw scores\n",
    "ax1.bar(['Class A', 'Class B', 'Class C'], raw_scores[0], alpha=0.7, color='lightblue', label='Raw Scores')\n",
    "ax1.set_title('Raw Scores Before Softmax')\n",
    "ax1.set_ylabel('Score Value')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# After softmax\n",
    "ax2.bar(['Class A', 'Class B', 'Class C'], softmax_probs[0], alpha=0.7, color='orange', label='Probabilities')\n",
    "ax2.set_title('Normalized Probabilities After Softmax')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Softmax Properties:**\n",
    "- **Normalization:** Outputs sum to 1\n",
    "- **Winner-takes-all:** Highest probability dominates\n",
    "- **Differentiable:** Enables gradient-based training\n",
    "\n",
    "**Healthcare Translation:** Like differential diagnosis - multiple possible diseases, probabilities indicate likelihood.\n",
    "\n",
    "**Reflection Question:** How does softmax help with uncertain diagnoses across multiple disease categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Our Implementation vs Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn baseline\n",
    "sk_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "sk_lr.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Our implementation predictions\n",
    "def predict_logistic(X, theta):\n",
    "    z = X @ theta\n",
    "    probabilities = sigmoid(z)\n",
    "    return (probabilities > 0.5).astype(int), probabilities\n",
    "\n",
    "our_predictions, our_probabilities = predict_logistic(X_test_b, theta_lr)\n",
    "sk_predictions = sk_lr.predict(X_test)\n",
    "sk_probabilities = sk_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nüéØ Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Accuracy\n",
    "our_accuracy = accuracy_score(y_test, our_predictions)\n",
    "sk_accuracy = accuracy_score(y_test, sk_predictions)\n",
    "print(f\"Accuracy - Our: {our_accuracy:.4f}, Sklearn: {sk_accuracy:.4f}\")\n",
    "\n",
    "# AUC\n",
    "our_auc = roc_auc_score(y_test, our_probabilities)\n",
    "sk_auc = roc_auc_score(y_test, sk_probabilities)\n",
    "print(f\"AUC-ROC - Our: {our_auc:.4f}, Sklearn: {sk_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nOur Implementation Classification Report:\")\n",
    "print(classification_report(y_test, our_predictions))\n",
    "\n",
    "print(\"\\nScikit-learn Classification Report:\")\n",
    "print(classification_report(y_test, sk_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy:** Overall correct predictions\n",
    "- **AUC-ROC:** Ability to distinguish classes\n",
    "- **Precision/Recall:** Class-specific performance\n",
    "\n",
    "**Healthcare Translation:** Like comparing diagnostic tools - accuracy matters, but so does avoiding false negatives.\n",
    "\n",
    "**Reflection Question:** Why is AUC-ROC important for medical diagnosis AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary Visualization\n",
    "\n",
    "Let's visualize how our logistic regression separates the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh grid for decision boundary\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Create feature matrix for mesh points\n",
    "mesh_features = np.c_[xx.ravel(), yy.ravel(), \n",
    "                     np.mean(X_train[:, 2:4], axis=0)[0] * np.ones(xx.ravel().shape[0]),\n",
    "                     np.mean(X_train[:, 2:4], axis=0)[1] * np.ones(xx.ravel().shape[0])]\n",
    "mesh_features_b = np.c_[np.ones((mesh_features.shape[0], 1)), mesh_features]\n",
    "\n",
    "# Predict on mesh\n",
    "mesh_predictions, _ = predict_logistic(mesh_features_b, theta_lr)\n",
    "mesh_predictions = mesh_predictions.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Decision boundary\n",
    "plt.contourf(xx, yy, mesh_predictions, alpha=0.4, cmap='RdYlBu')\n",
    "\n",
    "# Training data points\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train.ravel(), \n",
    "                     edgecolors='black', cmap='RdYlBu', s=50)\n",
    "\n",
    "# Test data points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test.ravel(), \n",
    "            marker='x', s=100, linewidth=2, cmap='RdYlBu')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic Regression Decision Boundary\\n(Blue = Class 0, Red = Class 1)')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_names = ['Intercept', 'Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\n",
    "importance = np.abs(theta_lr.flatten())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(feature_names, importance)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Absolute Weight Magnitude')\n",
    "plt.title('Feature Importance in Logistic Regression')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight most important feature\n",
    "max_idx = np.argmax(importance[1:]) + 1  # Skip intercept\n",
    "bars[max_idx].set_color('red')\n",
    "plt.legend([bars[max_idx]], ['Most Important Feature'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Decision Boundary:**\n",
    "- **Linear separation:** Logistic regression creates linear boundaries\n",
    "- **Probability regions:** Color intensity shows confidence\n",
    "- **Feature importance:** Shows which symptoms matter most\n",
    "\n",
    "**Healthcare Translation:** Like diagnostic guidelines - clear rules for classifying patients.\n",
    "\n",
    "**Reflection Question:** How might non-linear decision boundaries improve medical diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Nigerian Healthcare Applications\n",
    "\n",
    "**Algorithm Summary:**\n",
    "- **Sigmoid:** Converts linear outputs to probabilities\n",
    "- **Log Loss:** Penalizes confident wrong predictions\n",
    "- **Gradient Descent:** Optimizes classification parameters\n",
    "- **Softmax:** Extends to multiple disease categories\n",
    "\n",
    "**Healthcare Translation - Mark:**\n",
    "\n",
    "Imagine building AI for Nigerian hospitals:\n",
    "- **Binary classification:** Disease present/absent diagnosis\n",
    "- **Multiclass:** Malaria vs Typhoid vs COVID-19 differentiation\n",
    "- **Probability outputs:** Uncertainty quantification for doctors\n",
    "- **Feature importance:** Which symptoms are most diagnostic\n",
    "\n",
    "**Performance achieved:** Our implementation approaches industry standards!\n",
    "\n",
    "**Reflection Questions:**\n",
    "1. How would you adapt logistic regression for Nigerian disease patterns?\n",
    "2. When might you prefer logistic regression over more complex models?\n",
    "3. How does probability calibration help with medical decision-making?\n",
    "\n",
    "**Next Steps:**\n",
    "- Add regularization to prevent overfitting\n",
    "- Implement learning rate scheduling\n",
    "- Extend to neural network classifiers\n",
    "\n",
    "**üèÜ Excellent progress, my student! You've mastered probabilistic classification.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
