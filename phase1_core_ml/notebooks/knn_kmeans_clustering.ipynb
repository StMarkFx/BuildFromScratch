{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN, K-Means & Clustering from Scratch\n",
    "\n",
    "**Welcome, St. Mark!** Now we explore distance-based methods and unsupervised learning. Think of this as understanding how similar patients cluster together and how we can classify based on proximity.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "1. **K-Nearest Neighbors (KNN)** - Classification by proximity\n",
    "2. **K-Means Clustering** - Lloyd's algorithm for grouping data\n",
    "3. **Hierarchical Clustering** - Agglomerative clustering methods\n",
    "4. **Distance Metrics** - Euclidean, Manhattan, and cosine distances\n",
    "5. **Cluster Evaluation** - Silhouette scores and elbow method\n",
    "\n",
    "By the end, you'll understand how proximity drives both supervised and unsupervised learning.\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Distance-based methods find patterns through proximity:\n",
    "- **KNN:** Classify based on nearest neighbors' labels\n",
    "- **K-Means:** Group data into K clusters by minimizing within-cluster distances\n",
    "- **Hierarchical:** Build nested clusters through progressive merging\n",
    "- **Distance Metrics:** Different ways to measure \"closeness\"\n",
    "\n",
    "**Key Question:** How do we leverage proximity for both classification and clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Clustering-Friendly Dataset\n",
    "\n",
    "We'll use scikit-learn's `make_blobs` to create clear clusters for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, classification_report\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Create clustering dataset with clear groups\n",
    "X_clusters, y_clusters = make_blobs(n_samples=300,\n",
    "                                   centers=4,\n",
    "                                   cluster_std=1.5,\n",
    "                                   random_state=42)\n",
    "\n",
    "# Create classification dataset for KNN\n",
    "X_clf, y_clf = make_classification(n_samples=200,\n",
    "                                  n_features=2,\n",
    "                                  n_classes=3,\n",
    "                                  n_informative=2,\n",
    "                                  n_redundant=0,\n",
    "                                  n_clusters_per_class=1,\n",
    "                                  random_state=42)\n",
    "\n",
    "# Split classification data\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Clustering dataset: {X_clusters.shape[0]} samples, {len(np.unique(y_clusters))} true clusters\")\n",
    "print(f\"Classification dataset: {X_train_clf.shape[0]} train, {X_test_clf.shape[0]} test samples\")\n",
    "print(f\"Feature ranges - Clustering: [{X_clusters.min():.2f}, {X_clusters.max():.2f}]\")\n",
    "print(f\"Feature ranges - Classification: [{X_clf.min():.2f}, {X_clf.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:** We've prepared two datasets.\n",
    "\n",
    "- **Clustering data:** Clear groups for unsupervised learning\n",
    "- **Classification data:** Labeled data for KNN supervised learning\n",
    "- **Different scales:** Shows algorithms handle various data ranges\n",
    "\n",
    "**Healthcare Analogy:** Like analyzing patient groups - some data has known diagnoses (supervised), other data needs grouping discovery (unsupervised).\n",
    "\n",
    "**Reflection Question:** When would you use clustering vs classification in medical data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Distance Metrics - Measuring Proximity\n",
    "\n",
    "Different ways to measure how \"close\" two points are:\n",
    "\n",
    "- **Euclidean:** Straight-line distance (most common)\n",
    "- **Manhattan:** City block distance (taxicab metric)\n",
    "- **Cosine:** Angle-based similarity (normalized direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Euclidean (L2) distance between two points.\"\"\"\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    \"\"\"Manhattan (L1) distance between two points.\"\"\"\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Cosine similarity (converted to distance).\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    # Convert similarity to distance (1 - similarity)\n",
    "    return 1 - similarity\n",
    "\n",
    "def compute_distance_matrix(X, metric='euclidean'):\n",
    "    \"\"\"Compute pairwise distance matrix for all points.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    distance_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if metric == 'euclidean':\n",
    "                dist = euclidean_distance(X[i], X[j])\n",
    "            elif metric == 'manhattan':\n",
    "                dist = manhattan_distance(X[i], X[j])\n",
    "            elif metric == 'cosine':\n",
    "                dist = cosine_similarity(X[i], X[j])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown metric: {metric}\")\n",
    "            \n",
    "            distance_matrix[i, j] = dist\n",
    "            distance_matrix[j, i] = dist  # Symmetric\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "# Test distance metrics on sample points\n",
    "point_a = np.array([1, 2])\n",
    "point_b = np.array([4, 6])\n",
    "point_c = np.array([1, 6])  # Same x as A, same y as B\n",
    "\n",
    "points = [point_a, point_b, point_c]\n",
    "point_names = ['A(1,2)', 'B(4,6)', 'C(1,6)']\n",
    "\n",
    "print(\"Distance Metrics Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Points':<12} {'Euclidean':<10} {'Manhattan':<10} {'Cosine':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(points)):\n",
    "    for j in range(i + 1, len(points)):\n",
    "        eucl = euclidean_distance(points[i], points[j])\n",
    "        manh = manhattan_distance(points[i], points[j])\n",
    "        cos = cosine_similarity(points[i], points[j])\n",
    "        print(f\"{point_names[i]}‚Üí{point_names[j]:<6} {eucl:<10.3f} {manh:<10.3f} {cos:<10.3f}\")\n",
    "\n",
    "# Visualize distance metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot points\n",
    "for ax in axes:\n",
    "    ax.scatter([p[0] for p in points], [p[1] for p in points], s=100, c='red')\n",
    "    for i, name in enumerate(point_names):\n",
    "        ax.annotate(name, (points[i][0] + 0.1, points[i][1] + 0.1), fontsize=12)\n",
    "    ax.set_xlim(0, 5)\n",
    "    ax.set_ylim(0, 7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Euclidean circles\n",
    "axes[0].set_title('Euclidean Distance\\n(Straight-line)')\n",
    "circle_a = plt.Circle((1, 2), euclidean_distance(point_a, point_b), fill=False, color='blue', linestyle='--')\n",
    "axes[0].add_patch(circle_a)\n",
    "\n",
    "# Manhattan diamonds\n",
    "axes[1].set_title('Manhattan Distance\\n(City blocks)')\n",
    "diamond_points = np.array([[1, 2 + manhattan_distance(point_a, point_b)],\n",
    "                          [1 + manhattan_distance(point_a, point_b), 2],\n",
    "                          [1, 2 - manhattan_distance(point_a, point_b)],\n",
    "                          [1 - manhattan_distance(point_a, point_b), 2]])\n",
    "axes[1].plot(diamond_points[:, 0], diamond_points[:, 1], 'g--')\n",
    "\n",
    "# Cosine angles\n",
    "axes[2].set_title('Cosine Distance\\n(Angle-based)')\n",
    "# Draw vectors from origin\n",
    "for point, name in zip(points, point_names):\n",
    "    axes[2].arrow(0, 0, point[0], point[1], head_width=0.1, head_length=0.1, \n",
    "                  fc='blue', ec='blue', alpha=0.7)\n",
    "    axes[2].text(point[0] + 0.1, point[1] + 0.1, name, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Distance Metrics:**\n",
    "- **Euclidean:** Straight-line distance, sensitive to all dimensions equally\n",
    "- **Manhattan:** Grid-based distance, good for categorical features\n",
    "- **Cosine:** Direction-based, ignores magnitude, good for text/high-dim data\n",
    "\n",
    "**Healthcare Analogy:** Different ways doctors measure patient similarity - symptoms, demographics, or treatment response patterns.\n",
    "\n",
    "**Reflection Question:** Which distance metric would you choose for comparing patient symptoms vs genetic profiles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: K-Nearest Neighbors (KNN) Classification\n",
    "\n",
    "KNN classifies by majority vote of K nearest neighbors:\n",
    "\n",
    "1. Find K closest training points to test point\n",
    "2. Let neighbors \"vote\" on the class\n",
    "3. Assign most common class as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors classifier from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data.\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def _compute_distance(self, a, b):\n",
    "        \"\"\"Compute distance between two points.\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return euclidean_distance(a, b)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return manhattan_distance(a, b)\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            return cosine_similarity(a, b)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for test data.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for x_test in X:\n",
    "            # Compute distances to all training points\n",
    "            distances = [self._compute_distance(x_test, x_train) \n",
    "                        for x_train in self.X_train]\n",
    "            \n",
    "            # Get indices of k nearest neighbors\n",
    "            k_nearest_indices = np.argsort(distances)[:self.k]\n",
    "            \n",
    "            # Get labels of k nearest neighbors\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_nearest_indices]\n",
    "            \n",
    "            # Majority vote\n",
    "            majority_vote = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(majority_vote)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train and test our KNN\n",
    "knn = KNNClassifier(k=3, distance_metric='euclidean')\n",
    "knn.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "knn_predictions = knn.predict(X_test_clf)\n",
    "knn_accuracy = accuracy_score(y_test_clf, knn_predictions)\n",
    "\n",
    "print(f\"Our KNN (k=3) Test Accuracy: {knn_accuracy:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn\n",
    "sk_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sk_knn.fit(X_train_clf, y_train_clf)\n",
    "sk_predictions = sk_knn.predict(X_test_clf)\n",
    "sk_accuracy = accuracy_score(y_test_clf, sk_predictions)\n",
    "\n",
    "print(f\"Scikit-learn KNN Test Accuracy: {sk_accuracy:.4f}\")\n",
    "print(f\"Accuracy difference: {abs(knn_accuracy - sk_accuracy):.4f}\")\n",
    "\n",
    "# Test different k values\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_temp = KNNClassifier(k=k)\n",
    "    knn_temp.fit(X_train_clf, y_train_clf)\n",
    "    pred_temp = knn_temp.predict(X_test_clf)\n",
    "    acc_temp = accuracy_score(y_test_clf, pred_temp)\n",
    "    accuracies.append(acc_temp)\n",
    "    print(f\"K={k}: Test Accuracy = {acc_temp:.4f}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('K (Number of Neighbors)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('KNN: Effect of K on Classification Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**KNN Process:**\n",
    "- **Distance computation:** Find closest training points\n",
    "- **Neighbor selection:** Choose K most similar examples\n",
    "- **Majority voting:** Let similar examples decide the class\n",
    "\n",
    "**K Selection Trade-offs:**\n",
    "- **Small K:** Sensitive to noise, may overfit\n",
    "- **Large K:** Smoother decisions, may underfit\n",
    "\n",
    "**Healthcare Analogy:** Like consulting K similar patient cases to diagnose a new patient.\n",
    "\n",
    "**Reflection Question:** How does KNN's \"lazy learning\" differ from the \"eager learning\" of logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: K-Means Clustering - Lloyd's Algorithm\n",
    "\n",
    "K-Means groups data into K clusters by minimizing within-cluster distances:\n",
    "\n",
    "1. **Initialize:** Randomly place K cluster centers\n",
    "2. **Assign:** Each point belongs to nearest center\n",
    "3. **Update:** Move centers to mean of assigned points\n",
    "4. **Repeat:** Until centers stop moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    \"\"\"\n",
    "    K-Means clustering using Lloyd's algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, max_iter=100, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.centers = None\n",
    "        self.labels = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit K-Means clustering.\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize centers randomly from data points\n",
    "        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        self.centers = X[random_indices].copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Step 1: Assign each point to nearest center\n",
    "            distances = np.zeros((n_samples, self.n_clusters))\n",
    "            for i in range(n_samples):\n",
    "                for j in range(self.n_clusters):\n",
    "                    distances[i, j] = euclidean_distance(X[i], self.centers[j])\n",
    "            \n",
    "            self.labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Step 2: Update centers to mean of assigned points\n",
    "            new_centers = np.zeros_like(self.centers)\n",
    "            for cluster in range(self.n_clusters):\n",
    "                cluster_points = X[self.labels == cluster]\n",
    "                if len(cluster_points) > 0:\n",
    "                    new_centers[cluster] = np.mean(cluster_points, axis=0)\n",
    "                else:\n",
    "                    # Reinitialize empty clusters\n",
    "                    new_centers[cluster] = X[np.random.choice(n_samples)]\n",
    "            \n",
    "            # Check for convergence\n",
    "            center_shift = np.sum(np.abs(new_centers - self.centers))\n",
    "            self.centers = new_centers\n",
    "            \n",
    "            if center_shift < 1e-6:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster labels for new data.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        distances = np.zeros((n_samples, self.n_clusters))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                distances[i, j] = euclidean_distance(X[i], self.centers[j])\n",
    "        \n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "# Apply our K-Means to clustering data\n",
    "kmeans = KMeansClustering(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X_clusters)\n",
    "\n",
    "print(f\"Final cluster centers:\")\n",
    "for i, center in enumerate(kmeans.centers):\n",
    "    print(f\"Cluster {i}: ({center[0]:.2f}, {center[1]:.2f})\")\n",
    "\n",
    "# Compare with scikit-learn\n",
    "sk_kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "sk_labels = sk_kmeans.fit_predict(X_clusters)\n",
    "\n",
    "print(f\"\\nOur K-Means silhouette score: {silhouette_score(X_clusters, kmeans.labels):.4f}\")\n",
    "print(f\"Scikit-learn silhouette score: {silhouette_score(X_clusters, sk_labels):.4f}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Our implementation\n",
    "scatter1 = ax1.scatter(X_clusters[:, 0], X_clusters[:, 1], c=kmeans.labels, \n",
    "                      cmap='viridis', alpha=0.6, s=50)\n",
    "ax1.scatter(kmeans.centers[:, 0], kmeans.centers[:, 1], c='red', \n",
    "           marker='x', s=200, linewidth=3, label='Centers')\n",
    "ax1.set_title('Our K-Means Clustering')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scikit-learn\n",
    "scatter2 = ax2.scatter(X_clusters[:, 0], X_clusters[:, 1], c=sk_labels, \n",
    "                      cmap='viridis', alpha=0.6, s=50)\n",
    "ax2.scatter(sk_kmeans.cluster_centers_[:, 0], sk_kmeans.cluster_centers_[:, 1], \n",
    "           c='red', marker='x', s=200, linewidth=3, label='Centers')\n",
    "ax2.set_title('Scikit-learn K-Means')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**K-Means Process:**\n",
    "- **Expectation:** Assign points to nearest centers\n",
    "- **Maximization:** Update centers to cluster means\n",
    "- **Convergence:** Iterate until centers stabilize\n",
    "\n",
    "**Algorithm Properties:**\n",
    "- **Sensitive to initialization:** Random starts can give different results\n",
    "- **Assumes spherical clusters:** Works best with round, similar-sized groups\n",
    "- **Hard assignments:** Each point belongs to exactly one cluster\n",
    "\n",
    "**Healthcare Analogy:** Like grouping patients by symptom patterns - malaria patients cluster differently from typhoid patients.\n",
    "\n",
    "**Reflection Question:** Why might K-Means fail on elongated or irregular-shaped clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Hierarchical Clustering - Agglomerative Method\n",
    "\n",
    "Hierarchical clustering builds a tree of nested clusters:\n",
    "\n",
    "1. **Start:** Each point is its own cluster\n",
    "2. **Merge:** Combine closest clusters iteratively\n",
    "3. **Stop:** When desired number of clusters reached\n",
    "\n",
    "**Linkage Methods:** Single, Complete, Average, Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClustering:\n",
    "    \"\"\"\n",
    "    Agglomerative hierarchical clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, linkage='single'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.linkage = linkage  # 'single', 'complete', 'average'\n",
    "        self.labels = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Perform agglomerative clustering.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize: each point is its own cluster\n",
    "        clusters = [[i] for i in range(n_samples)]\n",
    "        \n",
    "        # Compute initial distance matrix\n",
    "        distance_matrix = compute_distance_matrix(X)\n",
    "        \n",
    "        while len(clusters) > self.n_clusters:\n",
    "            # Find closest pair of clusters\n",
    "            min_distance = float('inf')\n",
    "            merge_i, merge_j = -1, -1\n",
    "            \n",
    "            for i in range(len(clusters)):\n",
    "                for j in range(i + 1, len(clusters)):\n",
    "                    # Compute distance between clusters based on linkage\n",
    "                    cluster_dist = self._cluster_distance(clusters[i], clusters[j], \n",
    "                                                        distance_matrix)\n",
    "                    \n",
    "                    if cluster_dist < min_distance:\n",
    "                        min_distance = cluster_dist\n",
    "                        merge_i, merge_j = i, j\n",
    "            \n",
    "            # Merge the closest clusters\n",
    "            clusters[merge_i].extend(clusters[merge_j])\n",
    "            del clusters[merge_j]\n",
    "        \n",
    "        # Convert cluster lists to labels\n",
    "        self.labels = np.zeros(n_samples, dtype=int)\n",
    "        for cluster_id, cluster in enumerate(clusters):\n",
    "            for point_idx in cluster:\n",
    "                self.labels[point_idx] = cluster_id\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _cluster_distance(self, cluster_a, cluster_b, distance_matrix):\n",
    "        \"\"\"Compute distance between two clusters based on linkage method.\"\"\"\n",
    "        distances = []\n",
    "        for i in cluster_a:\n",
    "            for j in cluster_b:\n",
    "                distances.append(distance_matrix[i, j])\n",
    "        \n",
    "        if self.linkage == 'single':\n",
    "            return min(distances)  # Single linkage\n",
    "        elif self.linkage == 'complete':\n",
    "            return max(distances)  # Complete linkage\n",
    "        elif self.linkage == 'average':\n",
    "            return np.mean(distances)  # Average linkage\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown linkage: {self.linkage}\")\n",
    "\n",
    "# Apply hierarchical clustering\n",
    "hierarchical = HierarchicalClustering(n_clusters=4, linkage='average')\n",
    "hierarchical.fit(X_clusters)\n",
    "\n",
    "# Compare with scikit-learn\n",
    "sk_hierarchical = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
    "sk_hierarchical_labels = sk_hierarchical.fit_predict(X_clusters)\n",
    "\n",
    "print(f\"Our Hierarchical silhouette score: {silhouette_score(X_clusters, hierarchical.labels):.4f}\")\n",
    "print(f\"Scikit-learn silhouette score: {silhouette_score(X_clusters, sk_hierarchical_labels):.4f}\")\n",
    "\n",
    "# Test different linkage methods\n",
    "linkages = ['single', 'complete', 'average']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, linkage in enumerate(linkages):\n",
    "    hc = HierarchicalClustering(n_clusters=4, linkage=linkage)\n",
    "    hc.fit(X_clusters)\n",
    "    \n",
    "    scatter = axes[i].scatter(X_clusters[:, 0], X_clusters[:, 1], c=hc.labels, \n",
    "                              cmap='viridis', alpha=0.6, s=50)\n",
    "    axes[i].set_title(f'Hierarchical Clustering\\n{linkage.capitalize()} Linkage')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Linkage Methods:**\n",
    "- **Single:** Closest points between clusters (can create long chains)\n",
    "- **Complete:** Furthest points between clusters (creates compact clusters)\n",
    "- **Average:** Mean distance between all point pairs\n",
    "\n",
    "**Hierarchical Advantages:**\n",
    "- **No need to specify K initially:** Can cut tree at different levels\n",
    "- **Dendrogram:** Visual representation of clustering hierarchy\n",
    "- **Deterministic:** Same results every time (unlike K-Means)\n",
    "\n",
    "**Healthcare Analogy:** Like building a taxonomy of diseases - from specific symptoms to broad categories.\n",
    "\n",
    "**Reflection Question:** When would you prefer hierarchical clustering over K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Cluster Evaluation - Silhouette and Elbow Methods\n",
    "\n",
    "How do we know if our clustering is good?\n",
    "\n",
    "- **Silhouette Score:** Measures how similar points are to their cluster vs other clusters\n",
    "- **Elbow Method:** Finds optimal K by looking for \"bend\" in within-cluster distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_score_custom(X, labels):\n",
    "    \"\"\"\n",
    "    Compute average silhouette score for clustering quality.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    distance_matrix = compute_distance_matrix(X)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Points in same cluster as i\n",
    "        same_cluster = labels == labels[i]\n",
    "        same_cluster[i] = False  # Exclude self\n",
    "        \n",
    "        if np.sum(same_cluster) == 0:\n",
    "            continue  # Skip isolated points\n",
    "        \n",
    "        # Average distance to points in same cluster (cohesion)\n",
    "        a_i = np.mean(distance_matrix[i, same_cluster])\n",
    "        \n",
    "        # Average distance to points in other clusters (separation)\n",
    "        b_i_values = []\n",
    "        for cluster in np.unique(labels):\n",
    "            if cluster != labels[i]:\n",
    "                other_cluster = labels == cluster\n",
    "                if np.sum(other_cluster) > 0:\n",
    "                    b_i_values.append(np.mean(distance_matrix[i, other_cluster]))\n",
    "        \n",
    "        b_i = min(b_i_values) if b_i_values else a_i\n",
    "        \n",
    "        # Silhouette score for point i\n",
    "        s_i = (b_i - a_i) / max(a_i, b_i)\n",
    "        silhouette_scores.append(s_i)\n",
    "    \n",
    "    return np.mean(silhouette_scores) if silhouette_scores else 0\n",
    "\n",
    "def elbow_method(X, max_k=10):\n",
    "    \"\"\"Find optimal K using elbow method (within-cluster sum of squares).\"\"\"\n",
    "    wcss = []  # Within-cluster sum of squares\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans_temp = KMeansClustering(n_clusters=k, random_state=42)\n",
    "        kmeans_temp.fit(X)\n",
    "        \n",
    "        # Calculate WCSS\n",
    "        wcss_k = 0\n",
    "        for cluster in range(k):\n",
    "            cluster_points = X[kmeans_temp.labels == cluster]\n",
    "            if len(cluster_points) > 0:\n",
    "                # Sum of squared distances to cluster center\n",
    "                distances = [euclidean_distance(point, kmeans_temp.centers[cluster]) \n",
    "                           for point in cluster_points]\n",
    "                wcss_k += np.sum(np.square(distances))\n",
    "        \n",
    "        wcss.append(wcss_k)\n",
    "    \n",
    "    return wcss\n",
    "\n",
    "# Evaluate clustering quality\n",
    "silhouette_ours = silhouette_score_custom(X_clusters, kmeans.labels)\n",
    "silhouette_sk = silhouette_score(X_clusters, sk_labels)\n",
    "\n",
    "print(f\"Silhouette Scores:\")\n",
    "print(f\"Our K-Means: {silhouette_ours:.4f}\")\n",
    "print(f\"Scikit-learn: {silhouette_sk:.4f}\")\n",
    "\n",
    "# Elbow method for optimal K\n",
    "wcss_values = elbow_method(X_clusters, max_k=8)\n",
    "\n",
    "# Plot elbow curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Elbow plot\n",
    "k_values = range(1, 9)\n",
    "ax1.plot(k_values, wcss_values, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (K)')\n",
    "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "ax1.set_title('Elbow Method: Finding Optimal K')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='True K=4')\n",
    "ax1.legend()\n",
    "\n",
    "# Silhouette scores for different K\n",
    "silhouette_scores = []\n",
    "for k in range(2, 9):\n",
    "    kmeans_temp = KMeansClustering(n_clusters=k, random_state=42)\n",
    "    kmeans_temp.fit(X_clusters)\n",
    "    sil_score = silhouette_score_custom(X_clusters, kmeans_temp.labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "ax2.plot(range(2, 9), silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (K)')\n",
    "ax2.set_ylabel('Average Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis: Cluster Quality')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axvline(x=4, color='red', linestyle='--', alpha=0.7, label='True K=4')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal K from elbow (where curve starts to flatten)\n",
    "optimal_k_elbow = 4  # Visually determined from plot\n",
    "optimal_k_silhouette = np.argmax(silhouette_scores) + 2  # +2 because we start from k=2\n",
    "\n",
    "print(f\"\\nOptimal K suggestions:\")\n",
    "print(f\"Elbow method: K={optimal_k_elbow}\")\n",
    "print(f\"Silhouette method: K={optimal_k_silhouette}\")\n",
    "print(f\"True number of clusters: K=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Analysis:**\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **Silhouette Score:** Higher = better separated clusters (range: -1 to 1)\n",
    "- **WCSS:** Lower = tighter clusters, but decreases with more clusters\n",
    "- **Elbow Point:** Where adding clusters gives diminishing returns\n",
    "\n",
    "**Choosing K:**\n",
    "- **Elbow method:** Look for sharp bend in WCSS curve\n",
    "- **Silhouette:** Look for peak values\n",
    "- **Domain knowledge:** Sometimes we know the \"right\" number of groups\n",
    "\n",
    "**Healthcare Analogy:** Like validating disease categories - are malaria and typhoid truly distinct groups?\n",
    "\n",
    "**Reflection Question:** Why is it important to evaluate clustering when we don't have ground truth labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Nigerian Healthcare Applications\n",
    "\n",
    "**Algorithm Summary:**\n",
    "- **KNN:** Instance-based learning using proximity\n",
    "- **K-Means:** Centroid-based clustering with Lloyd's algorithm\n",
    "- **Hierarchical:** Tree-based clustering with different linkage methods\n",
    "- **Evaluation:** Silhouette scores and elbow method for quality assessment\n",
    "\n",
    "**Healthcare Translation - Mark:**\n",
    "\n",
    "Imagine building AI for Nigerian hospitals:\n",
    "- **KNN Diagnosis:** \"This patient is most similar to 3 malaria cases and 1 typhoid case\"\n",
    "- **Patient Clustering:** Group patients by symptom patterns to discover new disease subtypes\n",
    "- **Hierarchical Categories:** From specific symptoms (fever+headache) to broad conditions (infectious diseases)\n",
    "- **Quality Assessment:** Ensure patient groups are meaningful, not random\n",
    "\n",
    "**Performance achieved:** Our implementations approach industry standards!\n",
    "\n",
    "**Reflection Questions:**\n",
    "1. How would you use clustering to discover unknown disease patterns in Nigerian health data?\n",
    "2. When should you prefer KNN over other classification methods for medical diagnosis?\n",
    "3. How might distance metrics affect clustering of patients from different Nigerian regions?\n",
    "\n",
    "**Next Steps:**\n",
    "- Add support for different distance metrics in clustering\n",
    "- Implement K-Means++ initialization for better convergence\n",
    "- Explore density-based clustering (DBSCAN)\n",
    "\n",
    "**üèÜ Exceptional work, my student! You've mastered proximity-based learning from the ground up.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
